{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b765be",
   "metadata": {},
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b57322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import AWS SDK for Python\n",
    "import boto3\n",
    "\n",
    "# import json and sys\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bfc60",
   "metadata": {},
   "source": [
    "## Listing Foundation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate a bedrock client\n",
    "bedrock = boto3.client(\"bedrock\", region_name=\"us-east-1\")\n",
    "# List foundation models\n",
    "response = bedrock.list_foundation_models()\n",
    "\n",
    "# print it the answer in a pretty way\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401ad58",
   "metadata": {},
   "source": [
    "## Invoking models using invokeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f99a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate a bedrock-runtime client in us-east-1\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffdf55",
   "metadata": {},
   "source": [
    "### Amazon Titan Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a943b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"amazon.nova-lite-v1:0\", \n",
    "    body=json.dumps({\n",
    "        \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "        \"maxTokens\": 8192,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "}) )\n",
    "\n",
    "\n",
    "# Decode the response body and print it\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511b840",
   "metadata": {},
   "source": [
    "### Llama 3's instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the prompt in Llama 3's instruction format.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"meta.llama3-70b-instruct-v1:0\", \n",
    "    body=json.dumps({\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_gen_len\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "    })\n",
    ")\n",
    "\n",
    "# Decode the response body and print it\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61374b",
   "metadata": {},
   "source": [
    "### Mistral AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16007f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "formatted_prompt = f\"<s>[INST]{prompt}[/INST]\"\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"mistral.mistral-large-2402-v1:0\", \n",
    "    body=json.dumps({\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "    })\n",
    ")\n",
    "\n",
    "# Decode the response body and print it\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c7482",
   "metadata": {},
   "source": [
    "### Anthropic Claude v2 (NOT WORKING ANYMORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "formatted_prompt = f\"\\n\\nHuman: {prompt}\\n\\nAssistant:\"\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"anthropic.claude-v2:0\", \n",
    "    body=json.dumps({\n",
    "        \"max_tokens_to_sample\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "        \"prompt\": formatted_prompt,\n",
    "    })\n",
    ")\n",
    "\n",
    "# Decode the response body and print it\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c07bc",
   "metadata": {},
   "source": [
    "### Anthropic Claude Haiku 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"anthropic.claude-3-haiku-20240307-v1:0\", \n",
    "    body=json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    ")\n",
    "\n",
    "# Decode the response body and print it\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c5ce4",
   "metadata": {},
   "source": [
    "### Amazon Nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b199de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    modelId=\"amazon.nova-lite-v1:0\", \n",
    "    body=json.dumps({\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 512,\n",
    "            \"temperature\": 0.5\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"text\": prompt\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    ")\n",
    "\n",
    "# Decode the response body and print it\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264010fc",
   "metadata": {},
   "source": [
    "## Converse API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07224336",
   "metadata": {},
   "source": [
    "### Amazon Titan Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=\"amazon.titan-text-express-v1\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response output\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaadc54",
   "metadata": {},
   "source": [
    "### Llama 3's instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a322af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=\"meta.llama3-70b-instruct-v1:0\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response output\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1dd31b",
   "metadata": {},
   "source": [
    "### Mistral AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=\"mistral.mistral-large-2402-v1:0\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response output\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f2d3c",
   "metadata": {},
   "source": [
    "### Anthropic Claude Haiku 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d35030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=\"anthropic.claude-3-haiku-20240307-v1:0\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response output\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95839b",
   "metadata": {},
   "source": [
    "### Amazon Nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dab056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=\"amazon.nova-lite-v1:0\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response output\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354311a",
   "metadata": {},
   "source": [
    "## ConverseStream API\n",
    "Showing the entire output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse_stream(\n",
    "    modelId=\"amazon.nova-lite-v1:0\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 512,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for event in response[\"stream\"]:\n",
    "    print(json.dumps(event, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ab339",
   "metadata": {},
   "source": [
    "Showing only the text to see it as streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff493826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in a text of at least 500 words\"\n",
    "\n",
    "# Invoke the model using converse\n",
    "response = bedrock_runtime.converse_stream(\n",
    "    modelId=\"amazon.nova-lite-v1:0\", \n",
    "    inferenceConfig = {\n",
    "        \"maxTokens\": 1024,\n",
    "        \"temperature\": 0.5\n",
    "    },\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for event in response[\"stream\"]:\n",
    "    if \"contentBlockDelta\" in event:\n",
    "        chunk = event[\"contentBlockDelta\"]\n",
    "        sys.stdout.write(chunk[\"delta\"][\"text\"])\n",
    "        sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
